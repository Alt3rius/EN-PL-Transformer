{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSN-xasd6VOx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a59b958-97b6-493e-e035-d9c08477b753"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from google.colab import drive\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDPXJcBQ8CTg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zNTkZRX5ACf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c73fad51-8ffc-4f61-9aae-9407131bc7c1"
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRGQOnTn9htt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"/content/drive/My Drive/Transformers/europarl-v7.pl-en.en\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    europarl_en = f.read()\n",
        "\n",
        "with open(\"/content/drive/My Drive/Transformers/europarl-v7.pl-en.pl\", mode=\"r\", encoding=\"utf-8\") as f:\n",
        "    europarl_pl = f.read()\n",
        "\n",
        "corpus_en = europarl_en.split(\"\\n\")\n",
        "corpus_pl = europarl_pl.split(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5VOfdAV_TEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=1e13)\n",
        "tokenizer_pl = tfds.features.text.SubwordTextEncoder.build_from_corpus(corpus_pl, target_vocab_size=1e13)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeM9FYJACaUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size_en = tokenizer_en.vocab_size + 2\n",
        "vocab_size_pl = tokenizer_pl.vocab_size + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6BGBFnWCi83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = [[vocab_size_en - 2] + tokenizer_en.encode(sentence) + [vocab_size_en - 1] for sentence in corpus_en]\n",
        "outputs = [[vocab_size_pl - 2] + tokenizer_pl.encode(sentence) + [vocab_size_pl - 1] for sentence in corpus_pl]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNtPV0IwDF3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "max_length = 20\n",
        "idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent) > max_length]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n",
        "idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent) > max_length]\n",
        "for idx in reversed(idx_to_remove):\n",
        "    del inputs[idx]\n",
        "    del outputs[idx]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns1hvJ8rEAz4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, value=0, padding=\"post\", maxlen=max_length)\n",
        "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs, value=0, padding=\"post\", maxlen=max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giKZImj5EdaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "buffer_size = 20000\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
        "\n",
        "dataset = dataset.cache()\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size)\n",
        "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RLV9iQ-E_Ut",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PositionalEncoding(layers.Layer):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def get_angles(self, pos, i, d_model):\n",
        "        angles = 1/np.power(10000., (2*(i//2))/np.float32(d_model))\n",
        "        return pos*angles\n",
        "\n",
        "    def call(self, inputs):\n",
        "        seq_length = inputs.shape.as_list()[-2]\n",
        "        d_model = inputs.shape.as_list()[-1]\n",
        "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
        "                                 np.arange(d_model)[np.newaxis, :],\n",
        "                                 d_model)\n",
        "        \n",
        "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
        "        pos_encoding = angles[np.newaxis, ...]\n",
        "        return inputs + tf.cast(pos_encoding, tf.float32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLeFDOB-LB85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(queries, keys, values, mask):\n",
        "    product = tf.matmul(queries, keys, transpose_b=True)\n",
        "\n",
        "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
        "    scaled_product = product/tf.math.sqrt(tf.cast(keys_dim, tf.float32))\n",
        "\n",
        "    if mask is not None:\n",
        "        scaled_product += (mask * -1e9)\n",
        "\n",
        "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
        "\n",
        "    return attention\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trV1b-c3L0d1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, n_heads):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "        assert self.d_model % self.n_heads == 0\n",
        "\n",
        "        self.d_heads = self.d_model // self.n_heads\n",
        "\n",
        "        self.dense_q = layers.Dense(units=self.d_model)\n",
        "        self.dense_k = layers.Dense(units=self.d_model)\n",
        "        self.dense_v = layers.Dense(units=self.d_model)\n",
        "\n",
        "        self.output_ffn = layers.Dense(units=self.d_model)\n",
        "\n",
        "    def split_heads(self, inputs, batch_size):\n",
        "        shape = (batch_size,\n",
        "                 -1,\n",
        "                 self.n_heads,\n",
        "                 self.d_heads)\n",
        "        \n",
        "        splitted_inputs = tf.reshape(inputs, shape=shape)\n",
        "        return tf.transpose(splitted_inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, queries, keys, values, mask):\n",
        "        batch_size = tf.shape(queries)[0]\n",
        "        queries = self.dense_q(queries)\n",
        "        keys = self.dense_k(keys)\n",
        "        values = self.dense_v(values)\n",
        "\n",
        "        queries = self.split_heads(queries, batch_size)\n",
        "        keys = self.split_heads(keys, batch_size)\n",
        "        values = self.split_heads(values, batch_size)\n",
        "\n",
        "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
        "\n",
        "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
        "\n",
        "        outputs = self.output_ffn(concat_attention)\n",
        "\n",
        "        return outputs\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mnce6uxNUOz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, ffn_units, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.ffn_units=ffn_units\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        self.multi_head_attention = MultiHeadAttention(self.n_heads)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=self.ffn_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        attention = self.multi_head_attention(inputs,\n",
        "                                              inputs,\n",
        "                                              inputs,\n",
        "                                              mask)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention+inputs)\n",
        "\n",
        "        outputs = self.dense_1(attention)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_2(outputs, training=training)\n",
        "        outputs = self.norm_2(outputs+attention)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcFhskNnSmsi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    def __init__(self,\n",
        "                 n_layers,\n",
        "                 ffn_units,\n",
        "                 n_heads,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"encoder\"):\n",
        "        super().__init__(name=name)\n",
        "        self.n_layers = n_layers\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.enc_layers = [EncoderLayer(ffn_units, n_heads, dropout) for _ in range(self.n_layers)]\n",
        "\n",
        "\n",
        "    def call(self, inputs, mask, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training=training)\n",
        "        \n",
        "        for i in range(n_layers):\n",
        "            outputs = self.enc_layers[i](outputs, mask, training)\n",
        "        \n",
        "        return outputs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2P7Pi54a69j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, ffn_units, n_heads, dropout):\n",
        "        super().__init__()\n",
        "        self.ffn_units = ffn_units\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = dropout\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.d_model = input_shape[-1]\n",
        "\n",
        "        self.multi_head_attention_1 = MultiHeadAttention(self.n_heads)\n",
        "        self.dropout_1 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.multi_head_attention_2 = MultiHeadAttention(self.n_heads)\n",
        "        self.dropout_2 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dense_1 = layers.Dense(units=self.ffn_units, activation=\"relu\")\n",
        "        self.dense_2 = layers.Dense(units=self.d_model)\n",
        "        self.dropout_3 = layers.Dropout(rate=self.dropout)\n",
        "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
        "        attention = self.dropout_1(attention, training=training)\n",
        "        attention = self.norm_1(attention+inputs)\n",
        "\n",
        "        attention_2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2)\n",
        "        attention_2 = self.dropout_2(attention_2, training=training)\n",
        "        attention_2 = self.norm_2(attention_2+attention)\n",
        "\n",
        "        outputs = self.dense_1(attention_2)\n",
        "        outputs = self.dense_2(outputs)\n",
        "        outputs = self.dropout_3(outputs, training=training)\n",
        "        outputs = self.norm_3(outputs+attention_2)\n",
        "        \n",
        "        return outputs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwUIwX04dT6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, \n",
        "                 n_layers,\n",
        "                 ffn_units,\n",
        "                 n_heads,\n",
        "                 dropout,\n",
        "                 vocab_size,\n",
        "                 d_model,\n",
        "                 name=\"decoder\"):\n",
        "        super().__init__(name=name)\n",
        "        self.d_model = d_model\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding()\n",
        "        self.dropout = layers.Dropout(rate=dropout)\n",
        "        self.dec_layers = [DecoderLayer(ffn_units, n_heads, dropout) for _ in range(n_layers)]\n",
        "\n",
        "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
        "        outputs = self.embedding(inputs)\n",
        "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        outputs = self.pos_encoding(outputs)\n",
        "        outputs = self.dropout(outputs, training=training)\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            outputs = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training)\n",
        "\n",
        "        return outputs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQgd2a9CfeSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.models.Model):\n",
        "    def __init__(self,\n",
        "                 vocab_size_enc,\n",
        "                 vocab_size_dec,\n",
        "                 d_model,\n",
        "                 n_layers,\n",
        "                 ffn_units,\n",
        "                 n_heads,\n",
        "                 dropout,\n",
        "                 name=\"transformer\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "        self.encoder = Encoder(n_layers,\n",
        "                               ffn_units,\n",
        "                               n_heads,\n",
        "                               dropout,\n",
        "                               vocab_size_enc,\n",
        "                               d_model)\n",
        "        self.decoder = Decoder(n_layers,\n",
        "                               ffn_units,\n",
        "                               n_heads,\n",
        "                               dropout,\n",
        "                               vocab_size_dec,\n",
        "                               d_model)\n",
        "        \n",
        "        self.final_ffn = layers.Dense(units=vocab_size_dec)\n",
        "        \n",
        "    def create_padding_mask(self, seq):\n",
        "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
        "        \n",
        "    def create_lookahead_mask(self, seq):\n",
        "        seq_len = tf.shape(seq)[1]\n",
        "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "        return look_ahead_mask\n",
        "        \n",
        "    def call(self, enc_inputs, dec_inputs, training):\n",
        "        enc_mask = self.create_padding_mask(enc_inputs)\n",
        "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs), self.create_lookahead_mask(dec_inputs))\n",
        "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
        "            \n",
        "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
        "        dec_outputs = self.decoder(dec_inputs, enc_outputs, dec_mask_1, dec_mask_2, training)\n",
        "\n",
        "        outputs = self.final_ffn(dec_outputs)\n",
        "\n",
        "        return outputs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuE2hSMviC6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "d_model = 128\n",
        "n_layers = 4\n",
        "ffn_units = 512\n",
        "n_heads = 8\n",
        "dropout = 0.1\n",
        "\n",
        "transformer = Transformer(vocab_size_enc=vocab_size_en,\n",
        "                          vocab_size_dec=vocab_size_pl,\n",
        "                          d_model=d_model,\n",
        "                          n_layers=n_layers,\n",
        "                          ffn_units=ffn_units,\n",
        "                          n_heads=n_heads,\n",
        "                          dropout=dropout)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P53XuYUOi4tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
        "\n",
        "\n",
        "def loss_function(target, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
        "    loss_ = loss_object(target, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvpIR8tSkcEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.warmup_steps = warmup_steps\n",
        "    \n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(tf.cast(step, tf.float32))\n",
        "        arg2 = step * (self.warmup_steps**-1.5)\n",
        "\n",
        "        return tf.math.rsqrt(tf.cast(self.d_model, tf.float32)) * tf.math.minimum(arg1, arg2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D0q_Q51lP6s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFYkRee9liIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/Transformers/ckpt/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Checkpoint restored!\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LroeydPMmLFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 4\n",
        "for epoch in range(epochs):\n",
        "    print(\"Start of epoch {}\".format(epoch+1))\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
        "        dec_inputs = targets[:, :-1]\n",
        "        dec_outputs_real = targets[:, 1:]\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
        "            loss = loss_function(dec_outputs_real, predictions)\n",
        "\n",
        "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "        train_loss(loss)\n",
        "        train_accuracy(dec_outputs_real, predictions)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
        "        \n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1, ckpt_save_path))\n",
        "    print(\"Time taken for 1 epoch {}\".format(time.time()-start))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yfMC3qLPt0W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fP-gqnVJYED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer.save_weights(\"/content/drive/My Drive/Transformers/transformer_trained\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EipnK0vKTlv-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCwnr0DC0vND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "    inp_sentence = [vocab_size_en - 2] + tokenizer_en.encode(inp_sentence) + [vocab_size_en - 1]\n",
        "    encoder_inp = tf.expand_dims(inp_sentence, axis=0)\n",
        "    \n",
        "    output = tf.expand_dims([vocab_size_pl - 2], axis=0)\n",
        "    for _ in range(max_length):\n",
        "        predictions = transformer(encoder_inp, output, False)\n",
        "        prediction = predictions[:, -1:, :]\n",
        "\n",
        "        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.float32)\n",
        "\n",
        "        if predicted_id == vocab_size_pl-1:\n",
        "            return tf.squeeze(output, axis=0)\n",
        "        \n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "    return tf.squeeze(output, axis=0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VZ8Vtaa2MBy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "    output = evaluate(sentence).numpy()\n",
        "\n",
        "    predicted_sentence = encoder_pl.decode([i for i in output if i < vocab_size_pl-2])\n",
        "\n",
        "    print(\"Input sentence: {}\".format(sentence))\n",
        "    print(\"Translated sentence: {}\".format(predicted_sentence))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBtpz3v--Is2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate(\"This is a problem we have to solve\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}